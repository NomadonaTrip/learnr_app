"""
Mock OpenAI API Server for Offline Development

This service mimics the OpenAI API for local development without requiring:
- Internet connection
- OpenAI API key
- API costs

Endpoints:
- POST /v1/embeddings - Returns deterministic embeddings
- POST /v1/chat/completions - Returns canned chat responses
- GET /health - Health check

Usage:
    python server.py
    # Server runs at http://localhost:8001
"""

import hashlib
import numpy as np
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Union
import uvicorn
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Mock OpenAI API",
    description="Mock OpenAI API for offline development",
    version="1.0.0"
)

# =============================================================================
# Request/Response Models (matching OpenAI API structure)
# =============================================================================

class EmbeddingRequest(BaseModel):
    input: Union[str, List[str]]
    model: str = "text-embedding-3-large"
    encoding_format: str = "float"


class EmbeddingData(BaseModel):
    object: str = "embedding"
    embedding: List[float]
    index: int


class EmbeddingUsage(BaseModel):
    prompt_tokens: int
    total_tokens: int


class EmbeddingResponse(BaseModel):
    object: str = "list"
    data: List[EmbeddingData]
    model: str
    usage: EmbeddingUsage


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str = "gpt-4-turbo-preview"
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    top_p: float = 1.0
    n: int = 1
    stream: bool = False


class ChatChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str = "stop"


class ChatUsage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatChoice]
    usage: ChatUsage


# =============================================================================
# Mock Embedding Generation
# =============================================================================

def generate_deterministic_embedding(text: str, dimensions: int = 3072) -> List[float]:
    """
    Generate deterministic embedding from text using hash-based seeding.

    This ensures the same text always produces the same embedding,
    making it useful for testing and development.

    Args:
        text: Input text to embed
        dimensions: Embedding dimension (default: 3072 for text-embedding-3-large)

    Returns:
        List of floats representing the embedding vector
    """
    # Use SHA-256 hash of text as seed
    text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()
    seed = int(text_hash[:8], 16) % (2**32)  # Use first 8 chars as seed

    # Generate deterministic random vector
    np.random.seed(seed)
    embedding = np.random.randn(dimensions)

    # Normalize to unit length (like OpenAI embeddings)
    norm = np.linalg.norm(embedding)
    normalized_embedding = (embedding / norm).tolist()

    return normalized_embedding


# =============================================================================
# Canned Responses for Chat Completions
# =============================================================================

CANNED_EXPLANATIONS = [
    """This is a mock explanation for offline development.

**Why the correct answer is correct:**
The correct answer addresses the key concept being tested. In a production environment, this would be a detailed explanation generated by GPT-4.

**Why other options are incorrect:**
- Option A: Misses the fundamental principle
- Option B: Applies in different context
- Option C: Partially correct but incomplete

**BABOK Reference:** Section 3.2.4 (mock reference)

*This is a mock response. In production, GPT-4 generates detailed, context-specific explanations.*
""",
    """Here's a comprehensive explanation (mock mode):

The correct approach in this scenario involves understanding the underlying business analysis principles.

**Key Concepts:**
1. Requirements elicitation best practices
2. Stakeholder engagement strategies
3. Documentation standards

**Common Mistakes:**
- Rushing to solutions without analysis
- Ignoring stakeholder input
- Inadequate documentation

*Note: This is a mock response for offline development.*
""",
    """Mock explanation generated for offline development:

This question tests your understanding of business analysis fundamentals. The correct answer demonstrates proper application of BABOK knowledge areas.

**Analysis:**
The question scenario presents a typical business analysis challenge. The correct approach requires balancing stakeholder needs, business objectives, and available resources.

**Best Practices:**
- Systematic requirements gathering
- Stakeholder collaboration
- Iterative refinement

*In production, this would be a GPT-4 generated explanation specific to the question.*
"""
]


def generate_mock_chat_response(messages: List[ChatMessage]) -> str:
    """
    Generate canned chat response based on message content.

    Args:
        messages: List of chat messages

    Returns:
        Mock response string
    """
    # Get the last user message
    user_message = messages[-1].content if messages else ""

    # Select response based on message content hash (deterministic)
    message_hash = hashlib.md5(user_message.encode()).hexdigest()
    response_index = int(message_hash[:8], 16) % len(CANNED_EXPLANATIONS)

    return CANNED_EXPLANATIONS[response_index]


# =============================================================================
# API Endpoints
# =============================================================================

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "service": "mock-openai", "mode": "development"}


@app.post("/v1/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest):
    """
    Create embeddings (mock implementation).

    Generates deterministic embeddings for consistent testing.
    """
    logger.info(f"Generating mock embeddings for model: {request.model}")

    # Handle both string and list inputs
    inputs = [request.input] if isinstance(request.input, str) else request.input

    # Generate embeddings
    embeddings_data = []
    for idx, text in enumerate(inputs):
        embedding = generate_deterministic_embedding(text, dimensions=3072)
        embeddings_data.append(
            EmbeddingData(
                embedding=embedding,
                index=idx
            )
        )

    # Calculate token count (rough approximation)
    total_tokens = sum(len(text.split()) for text in inputs)

    response = EmbeddingResponse(
        data=embeddings_data,
        model=request.model,
        usage=EmbeddingUsage(
            prompt_tokens=total_tokens,
            total_tokens=total_tokens
        )
    )

    logger.info(f"Generated {len(embeddings_data)} embeddings")
    return response


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion(request: ChatCompletionRequest):
    """
    Create chat completion (mock implementation).

    Returns canned responses for offline development.
    """
    logger.info(f"Generating mock chat completion for model: {request.model}")

    if request.stream:
        raise HTTPException(
            status_code=400,
            detail="Streaming not supported in mock mode"
        )

    # Generate mock response
    response_content = generate_mock_chat_response(request.messages)

    # Calculate token counts (rough approximation)
    prompt_tokens = sum(len(msg.content.split()) for msg in request.messages)
    completion_tokens = len(response_content.split())

    import time
    response = ChatCompletionResponse(
        id=f"chatcmpl-mock-{int(time.time())}",
        created=int(time.time()),
        model=request.model,
        choices=[
            ChatChoice(
                index=0,
                message=ChatMessage(
                    role="assistant",
                    content=response_content
                )
            )
        ],
        usage=ChatUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens
        )
    )

    logger.info("Generated mock chat completion")
    return response


@app.get("/")
async def root():
    """Root endpoint with service information."""
    return {
        "service": "Mock OpenAI API",
        "mode": "development",
        "endpoints": {
            "embeddings": "POST /v1/embeddings",
            "chat": "POST /v1/chat/completions",
            "health": "GET /health"
        },
        "note": "This is a mock service for offline development. Responses are deterministic and canned."
    }


# =============================================================================
# Main
# =============================================================================

if __name__ == "__main__":
    logger.info("Starting Mock OpenAI API server...")
    logger.info("This service provides mock responses for offline development")
    logger.info("Access docs at: http://localhost:8001/docs")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        log_level="info"
    )
