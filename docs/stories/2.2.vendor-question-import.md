# Story 2.2: Vendor Question Import and Metadata Enrichment

## Status
Draft

## Story
**As a** content manager,
**I want** to import 500 vendor CBAP questions with metadata into PostgreSQL and Qdrant,
**so that** the platform has a high-quality question foundation.

## Acceptance Criteria
1. Questions table schema in PostgreSQL:
   - `questions` (id, question_text, option_a, option_b, option_c, option_d, correct_answer, explanation, ka, difficulty, concept_tags JSONB, source VARCHAR, created_at)
2. Python script `/scripts/import_vendor_questions.py` reads vendor questions from CSV/JSON
3. Script validates each question: Required fields present, exactly 4 options, correct_answer is A/B/C/D, KA is one of 6 valid KAs
4. Difficulty labels assigned by expert or default to "Medium" if not provided
5. Concept tags extracted or manually assigned (JSONB array in PostgreSQL)
6. Questions inserted into PostgreSQL `questions` table (500 total)
7. Distribution validation: Each KA has at least 50 questions, balanced across difficulty levels
8. Script logs summary: Total questions imported, breakdown by KA and difficulty
9. Rollback mechanism if import fails mid-process (transaction-based insert or idempotent script)
10. README documents how to run import script and expected CSV/JSON format

## Tasks / Subtasks

- [ ] **Task 1: Create Questions Table Schema** (AC: 1)
  - [ ] Create Alembic migration file: `migrations/versions/002_create_questions_table.py`
  - [ ] Define `questions` table with columns:
    - [ ] `id` UUID PRIMARY KEY DEFAULT gen_random_uuid()
    - [ ] `question_text` TEXT NOT NULL
    - [ ] `option_a` TEXT NOT NULL
    - [ ] `option_b` TEXT NOT NULL
    - [ ] `option_c` TEXT NOT NULL
    - [ ] `option_d` TEXT NOT NULL
    - [ ] `correct_answer` VARCHAR(1) NOT NULL CHECK (correct_answer IN ('A', 'B', 'C', 'D'))
    - [ ] `explanation` TEXT NOT NULL
    - [ ] `ka` VARCHAR(100) NOT NULL (knowledge_area)
    - [ ] `difficulty` VARCHAR(20) NOT NULL CHECK (difficulty IN ('Easy', 'Medium', 'Hard'))
    - [ ] `concept_tags` JSONB DEFAULT '[]'
    - [ ] `source` VARCHAR(50) NOT NULL DEFAULT 'vendor'
    - [ ] `babok_reference` VARCHAR(100)
    - [ ] `times_seen` INTEGER DEFAULT 0
    - [ ] `avg_correct_rate` FLOAT DEFAULT 0.0
    - [ ] `created_at` TIMESTAMP NOT NULL DEFAULT NOW()
    - [ ] `updated_at` TIMESTAMP NOT NULL DEFAULT NOW()
  - [ ] Add indexes:
    - [ ] CREATE INDEX idx_questions_ka ON questions(ka)
    - [ ] CREATE INDEX idx_questions_difficulty ON questions(difficulty)
    - [ ] CREATE INDEX idx_questions_source ON questions(source)
    - [ ] CREATE INDEX idx_questions_concept_tags ON questions USING GIN(concept_tags)
  - [ ] Run migration: `alembic upgrade head`
  - [ ] Verify table exists in PostgreSQL

- [ ] **Task 2: Create SQLAlchemy Question Model** (AC: 1)
  - [ ] Create `apps/api/src/models/question.py`
  - [ ] Define Question class with all table columns
  - [ ] Add relationship to responses (if responses table exists)
  - [ ] Define __repr__ method for debugging
  - [ ] Import in `apps/api/src/models/__init__.py`

- [ ] **Task 3: Create Question Repository** (AC: 6, 9)
  - [ ] Create `apps/api/src/repositories/question_repository.py`
  - [ ] Implement `async def create_question(question_data: dict) -> Question`
  - [ ] Implement `async def bulk_create_questions(questions: List[dict]) -> int` (transaction-based)
  - [ ] Implement `async def get_question_by_id(question_id: UUID) -> Question | None`
  - [ ] Implement `async def get_questions_by_ka(ka: str) -> List[Question]`
  - [ ] Implement `async def get_question_count_by_ka() -> Dict[str, int]`
  - [ ] Implement `async def get_question_count_by_difficulty() -> Dict[str, int]`
  - [ ] Add async transaction support for rollback

- [ ] **Task 4: Create Vendor Question Import Script** (AC: 2, 3, 4, 5, 6, 7, 8, 9)
  - [ ] Create `scripts/import_vendor_questions.py`
  - [ ] Add command-line arguments:
    - [ ] `--file` (path to CSV/JSON file, required)
    - [ ] `--format` (csv or json, default: csv)
    - [ ] `--dry-run` (validate without inserting)
    - [ ] `--verbose` (detailed logging)
  - [ ] Implement CSV parser (using csv module)
  - [ ] Implement JSON parser (using json module)
  - [ ] Validate each question:
    - [ ] Check all required fields present (question_text, option_a/b/c/d, correct_answer, explanation, ka)
    - [ ] Verify exactly 4 options (A, B, C, D)
    - [ ] Validate correct_answer is one of: A, B, C, D
    - [ ] Validate ka against valid KA list (6 knowledge areas from data model)
    - [ ] Assign difficulty (use provided or default to "Medium")
    - [ ] Parse concept_tags (comma-separated string → JSONB array)
    - [ ] Set source = "vendor"
  - [ ] Log validation errors (skip invalid questions, report at end)
  - [ ] Perform distribution validation:
    - [ ] Count questions by KA
    - [ ] Verify each KA has at least 50 questions
    - [ ] Log warning if imbalanced (not blocking)
  - [ ] Insert questions using bulk_create_questions (transaction-based)
  - [ ] Handle rollback on failure (SQLAlchemy transaction)
  - [ ] Log summary:
    - [ ] Total questions imported
    - [ ] Breakdown by KA (counts)
    - [ ] Breakdown by difficulty (counts)
    - [ ] Validation errors encountered
  - [ ] Make script idempotent (check if question already exists by question_text hash or unique constraint)

- [ ] **Task 5: Create Sample Vendor Question Data** (AC: 2, 6)
  - [ ] Create `scripts/data/vendor_questions_sample.csv`
  - [ ] Include 10 sample questions with all required fields
  - [ ] Cover at least 3 different KAs
  - [ ] Include all difficulty levels
  - [ ] Add concept_tags examples

- [ ] **Task 6: Write Import Script Tests** (AC: 3, 4, 5, 7, 9)
  - [ ] Create `apps/api/tests/unit/test_import_validation.py`
  - [ ] Test: Validate required fields (missing field raises error)
  - [ ] Test: Validate correct_answer is A/B/C/D (invalid value rejected)
  - [ ] Test: Validate KA is one of 6 valid KAs (invalid KA rejected)
  - [ ] Test: Difficulty defaults to "Medium" if not provided
  - [ ] Test: Concept tags parsed correctly (string → JSONB array)
  - [ ] Create `apps/api/tests/integration/test_question_repository.py`
  - [ ] Test: Bulk create questions (transaction commits)
  - [ ] Test: Rollback on error (transaction reverts)
  - [ ] Test: Distribution validation (50+ questions per KA)
  - [ ] Test: Idempotency (re-import doesn't duplicate)

- [ ] **Task 7: Update README with Import Instructions** (AC: 10)
  - [ ] Update `scripts/README.md` (or create if doesn't exist)
  - [ ] Document expected CSV format:
    - [ ] Column headers: question_text, option_a, option_b, option_c, option_d, correct_answer, explanation, ka, difficulty, concept_tags
    - [ ] Example row
  - [ ] Document expected JSON format (array of question objects)
  - [ ] Document how to run import script:
    - [ ] `python scripts/import_vendor_questions.py --file data/vendor_questions.csv`
    - [ ] `python scripts/import_vendor_questions.py --file data/vendor_questions.json --format json --dry-run`
  - [ ] Document validation rules
  - [ ] Document rollback behavior
  - [ ] Add troubleshooting section

- [ ] **Task 8: Verify Distribution and Data Quality** (AC: 7)
  - [ ] Run import script with full vendor question dataset
  - [ ] Query database to verify:
    - [ ] Total count = 500 questions
    - [ ] Each KA has at least 50 questions
    - [ ] Difficulty distribution is reasonable
    - [ ] Concept tags are valid JSONB arrays
    - [ ] All questions have 4 options and correct_answer

- [ ] **Task 9: Create Data Validation Utility Functions** (AC: 3)
  - [ ] Create `scripts/utils/validators.py`
  - [ ] Implement `validate_ka(ka: str) -> bool` (checks against 6 valid KAs)
  - [ ] Implement `validate_difficulty(difficulty: str) -> str` (returns normalized or "Medium")
  - [ ] Implement `validate_correct_answer(answer: str) -> bool`
  - [ ] Implement `parse_concept_tags(tags: str) -> List[str]`
  - [ ] Add unit tests for each validator

## Dev Notes

### Previous Story Context

From **Epic 1** (relevant learnings):
- PostgreSQL database set up via Docker Compose
- Alembic migrations established for schema versioning
- Repository pattern for data access (async methods)
- Environment variable management patterns

From **Story 2.1** (Qdrant Setup):
- Qdrant vector database running locally
- Questions will eventually need embeddings uploaded to Qdrant (Story 2.3)
- This story focuses ONLY on PostgreSQL import (no Qdrant operations yet)

### Valid Knowledge Areas (KAs)

[Source: architecture/data-models.md#Question]

The 6 CBAP Knowledge Areas that MUST be validated against:
1. "Business Analysis Planning and Monitoring"
2. "Elicitation and Collaboration"
3. "Requirements Life Cycle Management"
4. "Strategy Analysis"
5. "Requirements Analysis and Design Definition"
6. "Solution Evaluation"

**Validation Rule:** Every question's `ka` field must match one of these exact strings (case-sensitive).

### PostgreSQL Questions Table Schema

[Source: architecture/database-schema.md]

**Database:** PostgreSQL 15.x
**Table Name:** `questions`

**Schema Design:**
```sql
CREATE TABLE questions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    question_text TEXT NOT NULL,
    option_a TEXT NOT NULL,
    option_b TEXT NOT NULL,
    option_c TEXT NOT NULL,
    option_d TEXT NOT NULL,
    correct_answer VARCHAR(1) NOT NULL CHECK (correct_answer IN ('A', 'B', 'C', 'D')),
    explanation TEXT NOT NULL,
    ka VARCHAR(100) NOT NULL,  -- Knowledge Area
    difficulty VARCHAR(20) NOT NULL CHECK (difficulty IN ('Easy', 'Medium', 'Hard')),
    concept_tags JSONB DEFAULT '[]',  -- Array of concept strings
    source VARCHAR(50) NOT NULL DEFAULT 'vendor',  -- 'vendor' or 'llm_generated'
    babok_reference VARCHAR(100),
    times_seen INTEGER DEFAULT 0,
    avg_correct_rate FLOAT DEFAULT 0.0,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_questions_ka ON questions(ka);
CREATE INDEX idx_questions_difficulty ON questions(difficulty);
CREATE INDEX idx_questions_source ON questions(source);
CREATE INDEX idx_questions_concept_tags ON questions USING GIN(concept_tags);
```

**Key Design Decisions:**
- UUID primary keys for security and distributed systems compatibility
- JSONB for concept_tags to support flexible tagging and efficient querying
- CHECK constraints for data integrity (correct_answer, difficulty)
- GIN index on concept_tags JSONB column for fast filtering

### Project Structure

[Source: architecture/source-tree.md]

**Script Location:**
```
scripts/
├── import_vendor_questions.py      # This story's import script
├── data/
│   └── vendor_questions_sample.csv # Sample data for testing
└── utils/
    └── validators.py               # Validation utility functions
```

**Backend Models:**
```
apps/api/src/
├── models/
│   └── question.py                 # SQLAlchemy Question model
├── repositories/
│   └── question_repository.py      # Question data access layer
└── db/
    └── migrations/
        └── versions/
            └── 002_create_questions_table.py  # Alembic migration
```

### SQLAlchemy Question Model

[Source: architecture/data-models.md#Question, architecture/backend-architecture.md#Repository Pattern]

```python
# apps/api/src/models/question.py
from sqlalchemy import Column, String, Text, Integer, Float, TIMESTAMP, CheckConstraint
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.sql import func
import uuid

from apps.api.src.db.session import Base

class Question(Base):
    __tablename__ = "questions"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    question_text = Column(Text, nullable=False)
    option_a = Column(Text, nullable=False)
    option_b = Column(Text, nullable=False)
    option_c = Column(Text, nullable=False)
    option_d = Column(Text, nullable=False)
    correct_answer = Column(String(1), nullable=False)
    explanation = Column(Text, nullable=False)
    ka = Column(String(100), nullable=False)  # Knowledge Area
    difficulty = Column(String(20), nullable=False)
    concept_tags = Column(JSONB, default=[])
    source = Column(String(50), nullable=False, default="vendor")
    babok_reference = Column(String(100), nullable=True)
    times_seen = Column(Integer, default=0)
    avg_correct_rate = Column(Float, default=0.0)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.now(), onupdate=func.now())

    __table_args__ = (
        CheckConstraint("correct_answer IN ('A', 'B', 'C', 'D')", name="check_correct_answer"),
        CheckConstraint("difficulty IN ('Easy', 'Medium', 'Hard')", name="check_difficulty"),
    )

    def __repr__(self):
        return f"<Question(id={self.id}, ka={self.ka}, difficulty={self.difficulty})>"
```

### Question Repository Pattern

[Source: architecture/backend-architecture.md#Repository Pattern]

```python
# apps/api/src/repositories/question_repository.py
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from sqlalchemy.exc import SQLAlchemyError
from typing import List, Dict
from uuid import UUID
import logging

from apps.api.src.models.question import Question

logger = logging.getLogger(__name__)

class QuestionRepository:
    def __init__(self, db: AsyncSession):
        self.db = db

    async def create_question(self, question_data: dict) -> Question:
        """
        Create a single question.

        Args:
            question_data: Dictionary with question fields

        Returns:
            Created Question instance
        """
        question = Question(**question_data)
        self.db.add(question)
        await self.db.commit()
        await self.db.refresh(question)
        logger.info(f"Created question: {question.id}")
        return question

    async def bulk_create_questions(self, questions: List[dict]) -> int:
        """
        Bulk insert questions with transaction support.

        Args:
            questions: List of question dictionaries

        Returns:
            Number of questions inserted

        Raises:
            SQLAlchemyError: If transaction fails (triggers rollback)
        """
        try:
            async with self.db.begin():  # Transaction
                question_objects = [Question(**q) for q in questions]
                self.db.add_all(question_objects)
                await self.db.flush()
                count = len(question_objects)
                logger.info(f"Bulk inserted {count} questions")
                return count
        except SQLAlchemyError as e:
            logger.error(f"Bulk insert failed: {str(e)}")
            await self.db.rollback()
            raise

    async def get_question_by_id(self, question_id: UUID) -> Question | None:
        """Retrieve question by ID."""
        result = await self.db.execute(
            select(Question).where(Question.id == question_id)
        )
        return result.scalar_one_or_none()

    async def get_questions_by_ka(self, ka: str) -> List[Question]:
        """Retrieve all questions for a specific knowledge area."""
        result = await self.db.execute(
            select(Question).where(Question.ka == ka)
        )
        return result.scalars().all()

    async def get_question_count_by_ka(self) -> Dict[str, int]:
        """
        Get question count grouped by knowledge area.

        Returns:
            Dictionary mapping KA name to count
        """
        result = await self.db.execute(
            select(Question.ka, func.count(Question.id))
            .group_by(Question.ka)
        )
        return {ka: count for ka, count in result.all()}

    async def get_question_count_by_difficulty(self) -> Dict[str, int]:
        """
        Get question count grouped by difficulty.

        Returns:
            Dictionary mapping difficulty to count
        """
        result = await self.db.execute(
            select(Question.difficulty, func.count(Question.id))
            .group_by(Question.difficulty)
        )
        return {difficulty: count for difficulty, count in result.all()}
```

### Import Script Structure

[Source: architecture/coding-standards.md]

**Coding Standards:**
- Python function names: `snake_case`
- Use async/await for database operations
- Use SQLAlchemy ORM (no raw SQL)
- Comprehensive logging for debugging
- Type hints for function parameters and returns

**Import Script Template:**
```python
# scripts/import_vendor_questions.py
import argparse
import csv
import json
import asyncio
import logging
from pathlib import Path
from typing import List, Dict
import sys

# Add project root to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from apps.api.src.db.session import AsyncSessionLocal
from apps.api.src.repositories.question_repository import QuestionRepository
from scripts.utils.validators import validate_ka, validate_difficulty, validate_correct_answer, parse_concept_tags

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

VALID_KAS = [
    "Business Analysis Planning and Monitoring",
    "Elicitation and Collaboration",
    "Requirements Life Cycle Management",
    "Strategy Analysis",
    "Requirements Analysis and Design Definition",
    "Solution Evaluation"
]

async def import_questions(file_path: str, format: str, dry_run: bool) -> None:
    """
    Import vendor questions from CSV or JSON file.

    Args:
        file_path: Path to input file
        format: 'csv' or 'json'
        dry_run: If True, validate without inserting
    """
    # Parse file
    questions = parse_file(file_path, format)

    # Validate questions
    valid_questions, errors = validate_questions(questions)

    # Log validation results
    logger.info(f"Validated {len(valid_questions)} questions")
    logger.info(f"Validation errors: {len(errors)}")

    if errors:
        for error in errors:
            logger.warning(f"Validation error: {error}")

    # Check distribution
    check_distribution(valid_questions)

    if dry_run:
        logger.info("DRY RUN - No questions inserted")
        return

    # Insert questions
    async with AsyncSessionLocal() as db:
        repo = QuestionRepository(db)
        try:
            count = await repo.bulk_create_questions(valid_questions)
            logger.info(f"Successfully imported {count} questions")

            # Log summary
            log_summary(valid_questions)
        except Exception as e:
            logger.error(f"Import failed: {str(e)}")
            raise

def parse_file(file_path: str, format: str) -> List[dict]:
    """Parse CSV or JSON file."""
    # Implementation here
    pass

def validate_questions(questions: List[dict]) -> tuple[List[dict], List[str]]:
    """Validate questions and return valid ones + errors."""
    # Implementation here
    pass

def check_distribution(questions: List[dict]) -> None:
    """Check that each KA has at least 50 questions."""
    # Implementation here
    pass

def log_summary(questions: List[dict]) -> None:
    """Log summary by KA and difficulty."""
    # Implementation here
    pass

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Import vendor CBAP questions")
    parser.add_argument("--file", required=True, help="Path to CSV or JSON file")
    parser.add_argument("--format", default="csv", choices=["csv", "json"], help="File format")
    parser.add_argument("--dry-run", action="store_true", help="Validate without inserting")
    parser.add_argument("--verbose", action="store_true", help="Detailed logging")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    asyncio.run(import_questions(args.file, args.format, args.dry_run))
```

### Expected CSV/JSON Format

**CSV Format:**
```csv
question_text,option_a,option_b,option_c,option_d,correct_answer,explanation,ka,difficulty,concept_tags
"What is the primary purpose of stakeholder analysis?","Identify stakeholders","Document requirements","Manage risks","Allocate resources","A","Stakeholder analysis identifies all parties affected by or affecting the project","Business Analysis Planning and Monitoring","Medium","stakeholder,analysis,planning"
```

**JSON Format:**
```json
[
  {
    "question_text": "What is the primary purpose of stakeholder analysis?",
    "option_a": "Identify stakeholders",
    "option_b": "Document requirements",
    "option_c": "Manage risks",
    "option_d": "Allocate resources",
    "correct_answer": "A",
    "explanation": "Stakeholder analysis identifies all parties affected by or affecting the project",
    "ka": "Business Analysis Planning and Monitoring",
    "difficulty": "Medium",
    "concept_tags": "stakeholder,analysis,planning"
  }
]
```

### Validation Rules

[Source: Epic 2.2 AC #3, #4, #5]

**Required Fields:** question_text, option_a, option_b, option_c, option_d, correct_answer, explanation, ka

**Validation Logic:**
1. **Required Fields:** All must be non-empty strings
2. **Exactly 4 Options:** option_a, option_b, option_c, option_d must all exist
3. **Correct Answer:** Must be one of: 'A', 'B', 'C', 'D' (case-insensitive, normalized to uppercase)
4. **Knowledge Area:** Must match one of 6 valid KAs (exact string match, case-sensitive)
5. **Difficulty:** If provided, must be 'Easy', 'Medium', or 'Hard'. If not provided, default to "Medium"
6. **Concept Tags:** Parse comma-separated string into JSONB array (e.g., "planning,stakeholder" → ["planning", "stakeholder"])

**Distribution Validation:**
- Each KA should have at least 50 questions (log warning if not met, but don't block import)

### Rollback Mechanism

[Source: architecture/backend-architecture.md#Repository Pattern, Epic 2.2 AC #9]

**Transaction-Based Insert:**
- Use SQLAlchemy's `async with db.begin()` for atomic transactions
- If any error occurs during bulk insert, entire transaction rolls back
- No partial imports (all 500 questions inserted or none)

**Idempotency:**
- Option 1: Add unique constraint on `question_text` hash
- Option 2: Check if question exists before inserting (slower but simpler)
- Script can be re-run without duplicating questions

### Testing Requirements

[Source: architecture/testing-strategy.md]

**Test Organization:**
```
apps/api/tests/
├── unit/
│   ├── test_import_validation.py    # Validation logic tests
│   └── repositories/
│       └── test_question_repository.py  # Repository tests
└── integration/
    └── test_question_import.py      # Full import workflow test
```

**Test Coverage:**
- Unit tests: Validation functions (validate_ka, validate_difficulty, etc.)
- Integration tests: Bulk insert, transaction rollback, idempotency
- Test fixtures: Sample CSV/JSON files with valid and invalid data

**Testing Framework:** pytest 7.4.x with async support (pytest-asyncio)

### Logging Requirements

[Source: Epic 2.2 AC #8]

**Log Summary Format:**
```
INFO: Starting import from vendor_questions.csv
INFO: Parsed 500 questions from file
INFO: Validated 495 questions (5 validation errors)
WARNING: Validation error on row 23: Invalid KA 'Business Planning'
INFO: Distribution check:
  - Business Analysis Planning and Monitoring: 85 questions
  - Elicitation and Collaboration: 78 questions
  - Requirements Life Cycle Management: 82 questions
  - Strategy Analysis: 75 questions
  - Requirements Analysis and Design Definition: 90 questions
  - Solution Evaluation: 85 questions
INFO: All KAs meet minimum 50 question threshold
INFO: Difficulty distribution:
  - Easy: 150 questions (30.3%)
  - Medium: 220 questions (44.4%)
  - Hard: 125 questions (25.3%)
INFO: Successfully imported 495 questions
```

### Coding Standards Reminders

[Source: architecture/coding-standards.md]

- **No Raw SQL:** Use SQLAlchemy ORM exclusively
- **Async/Await:** All database operations must be async
- **Type Hints:** Use for function parameters and returns
- **Error Handling:** Catch and log SQLAlchemyError, raise for caller to handle
- **Logging:** Use structured logging with context (question count, KA, etc.)

### Dependencies to Add

[Source: architecture/tech-stack.md]

```txt
# Already in apps/api/requirements.txt from Epic 1:
SQLAlchemy==2.0.x
alembic==1.13.x
asyncpg==0.29.x  # PostgreSQL async driver

# No new dependencies required for this story
```

### Success Criteria Checklist

Before marking this story complete, verify:
1. ✓ Questions table created in PostgreSQL with all columns and indexes
2. ✓ SQLAlchemy Question model defined and importable
3. ✓ QuestionRepository with bulk_create_questions method working
4. ✓ Import script reads CSV and JSON files correctly
5. ✓ Validation logic rejects invalid questions (correct_answer, KA, etc.)
6. ✓ Difficulty defaults to "Medium" when not provided
7. ✓ Concept tags parsed into JSONB array
8. ✓ Distribution validation checks 50+ questions per KA
9. ✓ Transaction rollback works on error
10. ✓ Script logs summary (total, by KA, by difficulty)
11. ✓ README documents CSV/JSON format and usage
12. ✓ Unit and integration tests pass
13. ✓ Import script successfully imports 500 vendor questions

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-21 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
